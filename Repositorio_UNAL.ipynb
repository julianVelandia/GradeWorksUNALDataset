{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMBEbQj2qWeTj2DcwAn7e2E",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/julianVelandia/GradeWorksUNALDataset/blob/master/Repositorio_UNAL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pdfplumber\n",
        "!pip install python-certifi-win32\n",
        "!pip install certifi\n",
        "!pip install --upgrade certifi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rz7HM2xolX6E",
        "outputId": "c42405d8-85a0-4760-e116-5265f10500a9"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pdfplumber\n",
            "  Downloading pdfplumber-0.10.2-py3-none-any.whl (47 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/47.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.5/47.5 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pdfminer.six==20221105 (from pdfplumber)\n",
            "  Downloading pdfminer.six-20221105-py3-none-any.whl (5.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m37.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: Pillow>=9.1 in /usr/local/lib/python3.10/dist-packages (from pdfplumber) (9.4.0)\n",
            "Collecting pypdfium2>=4.18.0 (from pdfplumber)\n",
            "  Downloading pypdfium2-4.20.0-py3-none-manylinux_2_17_x86_64.whl (3.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m48.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from pdfminer.six==20221105->pdfplumber) (3.2.0)\n",
            "Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.10/dist-packages (from pdfminer.six==20221105->pdfplumber) (41.0.3)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography>=36.0.0->pdfminer.six==20221105->pdfplumber) (1.15.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six==20221105->pdfplumber) (2.21)\n",
            "Installing collected packages: pypdfium2, pdfminer.six, pdfplumber\n",
            "Successfully installed pdfminer.six-20221105 pdfplumber-0.10.2 pypdfium2-4.20.0\n",
            "Collecting python-certifi-win32\n",
            "  Downloading python_certifi_win32-1.6.1-py2.py3-none-any.whl (7.3 kB)\n",
            "Requirement already satisfied: wrapt>=1.10.4 in /usr/local/lib/python3.10/dist-packages (from python-certifi-win32) (1.15.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from python-certifi-win32) (2023.7.22)\n",
            "Collecting setuptools-scm (from python-certifi-win32)\n",
            "  Downloading setuptools_scm-7.1.0-py3-none-any.whl (43 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.8/43.8 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from setuptools-scm->python-certifi-win32) (23.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from setuptools-scm->python-certifi-win32) (67.7.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from setuptools-scm->python-certifi-win32) (4.5.0)\n",
            "Requirement already satisfied: tomli>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from setuptools-scm->python-certifi-win32) (2.0.1)\n",
            "Installing collected packages: setuptools-scm, python-certifi-win32\n",
            "Successfully installed python-certifi-win32-1.6.1 setuptools-scm-7.1.0\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (2023.7.22)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (2023.7.22)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "fy8xqTyRk507"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from io import BytesIO\n",
        "import pdfplumber\n",
        "import ssl\n",
        "from bs4 import BeautifulSoup\n",
        "import json\n",
        "import os\n",
        "import re\n",
        "from typing import List, Dict"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def save_uri_in_json(uri):\n",
        "  file_name = 'uris.json'\n",
        "  if os.path.exists(file_name):\n",
        "      with open(file_name, 'r', encoding='utf-8') as json_file:\n",
        "          data = json.load(json_file)\n",
        "  else:\n",
        "      data = {\"uris\": {}}\n",
        "\n",
        "\n",
        "  data[\"uris\"][uri] = \"pending\"\n",
        "\n",
        "  with open(file_name, 'w', encoding='utf-8') as json_file:\n",
        "      json.dump(data, json_file, indent=4, ensure_ascii=False)"
      ],
      "metadata": {
        "id": "nfcUZk491E3T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_metadata_in_json(uri: str, data_dict: Dict[str, dict]):\n",
        "    file_name = 'dataset.json'\n",
        "\n",
        "    try:\n",
        "        try:\n",
        "            with open(file_name, 'r', encoding='utf-8') as json_file:\n",
        "                existing_data = json.load(json_file)\n",
        "        except (FileNotFoundError, json.JSONDecodeError):\n",
        "            existing_data = {}\n",
        "\n",
        "\n",
        "        existing_data[uri] = data_dict\n",
        "\n",
        "        with open(file_name, 'w',  encoding='utf-8') as json_file:\n",
        "            json.dump(existing_data, json_file, indent=4, ensure_ascii=False)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"ERROR: {e}\")"
      ],
      "metadata": {
        "id": "bQLFTCLy1BZc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_uris_pdfs_and_metadata_from_repositorio_unal():\n",
        "  base_url = \"https://repositorio.unal.edu.co/handle/unal/5/recent-submissions?offset=\"\n",
        "  offset = 0\n",
        "  url_list = []\n",
        "  while offset <= 1900:\n",
        "      url = base_url + str(offset)\n",
        "      print('OFFSET: '+ str(offset))\n",
        "      offset += 20\n",
        "      try:\n",
        "        response = requests.get(url, verify=False)\n",
        "        soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "        artifact_items = soup.find_all(class_=\"ds-artifact-item\")\n",
        "\n",
        "        for item in artifact_items:\n",
        "            item_wrapper = item.find('div', class_=\"item-wrapper row\")\n",
        "            if item_wrapper:\n",
        "                col_div = item_wrapper.find('div', class_=\"col-sm-3 hidden-xs\")\n",
        "\n",
        "                if col_div:\n",
        "                    thumbnail_div = col_div.find('div', class_=\"thumbnail artifact-preview\")\n",
        "                    if thumbnail_div:\n",
        "                        a_tag = thumbnail_div.find('a', class_=\"image-link\")\n",
        "\n",
        "                        if a_tag and 'href' in a_tag.attrs:\n",
        "                            item_page_url = a_tag['href']\n",
        "                            item_page_url = item_page_url.strip()\n",
        "                            item_page_url = 'https://repositorio.unal.edu.co' + item_page_url + '?show=full'\n",
        "                            item_page_response = requests.get(item_page_url, verify=False)\n",
        "                            item_page_soup = BeautifulSoup(item_page_response.content, 'html.parser')\n",
        "\n",
        "                            advisor_label = item_page_soup.find('td', class_='label-cell', string='dc.contributor.advisor')\n",
        "                            if advisor_label:\n",
        "                                advisor_row = advisor_label.find_parent('tr')\n",
        "                                advisor = str(advisor_row.find_all('td', class_='word-break')[0]).replace('<td class=\"word-break\">', '').replace('</td>','')\n",
        "\n",
        "\n",
        "                            author_label = item_page_soup.find('td', class_='label-cell', string='dc.contributor.author')\n",
        "                            if author_label:\n",
        "                                author_row = author_label.find_parent('tr')\n",
        "                                author = str(author_row.find_all('td', class_='word-break')[0]).replace('<td class=\"word-break\">', '').replace('</td>','')\n",
        "\n",
        "\n",
        "                            date_label = item_page_soup.find('td', class_='label-cell', string='dc.date.issued')\n",
        "                            if date_label:\n",
        "                                date_row = date_label.find_parent('tr')\n",
        "                                date = str(date_row.find_all('td', class_='word-break')[0]).replace('<td class=\"word-break\">', '').replace('</td>','')\n",
        "\n",
        "                            description_label = item_page_soup.find('td', class_='label-cell', string='dc.description.abstract')\n",
        "                            if description_label:\n",
        "                                description_row = description_label.find_parent('tr')\n",
        "                                description = str(description_row.find_all('td', class_='word-break')[0]).replace('<td class=\"word-break\">', '').replace('</td>','')\n",
        "\n",
        "                            title_label = item_page_soup.find('td', class_='label-cell', string='dc.title')\n",
        "                            if title_label:\n",
        "                                title_row = title_label.find_parent('tr')\n",
        "                                title = str(title_row.find_all('td', class_='word-break')[0]).replace('<td class=\"word-break\">', '').replace('</td>','')\n",
        "\n",
        "\n",
        "                            program_label = item_page_soup.find('td', class_='label-cell', string='dc.publisher.program')\n",
        "                            if program_label:\n",
        "                                program_row = program_label.find_parent('tr')\n",
        "                                program = str(program_row.find_all('td', class_='word-break')[0]).replace('<td class=\"word-break\">', '').replace('</td>','')\n",
        "\n",
        "\n",
        "                            faculty_label = item_page_soup.find('td', class_='label-cell', string='dc.publisher.faculty')\n",
        "                            if faculty_label:\n",
        "                                faculty_row = faculty_label.find_parent('tr')\n",
        "                                faculty = str(faculty_row.find_all('td', class_='word-break')[0]).replace('<td class=\"word-break\">', '').replace('</td>','')\n",
        "\n",
        "\n",
        "                            data_dict = {\n",
        "                                \"advisor\": advisor,\n",
        "                                \"author\": author,\n",
        "                                \"date\": date,\n",
        "                                \"description\": description,\n",
        "                                \"title\": title,\n",
        "                                \"program\": program,\n",
        "                                \"faculty\": faculty\n",
        "                            }\n",
        "                            url_element = item_page_soup.select_one('.thumbnail a')\n",
        "                            url_key =''\n",
        "                            if url_element:\n",
        "                                url_key = url_element.get('href')\n",
        "                            else:\n",
        "\n",
        "                              continue\n",
        "\n",
        "\n",
        "                            save_uri_in_json(url_key)\n",
        "                            save_metadata_in_json(url_key, data_dict)\n",
        "\n",
        "\n",
        "\n",
        "      except Exception as e:\n",
        "        print('ERROR: '+url + str(e))\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "9o55Ko1QY-0h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "SUBIR ARCHIVOS dataset.json y uris.json"
      ],
      "metadata": {
        "id": "UQv7gCPgFmyk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "  if not os.path.exists('dataset.json') and not os.path.exists('uris.json'):\n",
        "      save_uris_pdfs_and_metadata_from_repositorio_unal()"
      ],
      "metadata": {
        "id": "lOBKVDtK1ZY7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Se deberían tener 2 archivos:\n",
        "- Dataset.json\n",
        " - Lista de la información preliminar de la tesis (advisor, author, date, description, rtitle, program, faculty), con la uri como llave\n",
        "- uris.json\n",
        " - Lista de las uris (que son las keys de dataset) y un estado pending"
      ],
      "metadata": {
        "id": "CtSeGcRzI5_w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def from_url_to_txt(pdf_url):\n",
        "  try:\n",
        "    response = requests.get(pdf_url, verify=False)\n",
        "    response.raise_for_status()\n",
        "    pdf_stream = pdfplumber.open(BytesIO(response.content))\n",
        "    pdf_text = \"\"\n",
        "    for page in pdf_stream.pages:\n",
        "      page_text = page.extract_text()\n",
        "      pdf_text += page_text\n",
        "    pdf_stream.close()\n",
        "  except:\n",
        "    print('ERROR: '+pdf_url)\n",
        "  return pdf_text"
      ],
      "metadata": {
        "id": "oBP4RkLjkIke"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_content_in_json(uri, raw_content):\n",
        "  file_name = 'raw_dataset.json'\n",
        "  if os.path.exists(file_name):\n",
        "      with open(file_name, 'r', encoding='utf-8') as json_file:\n",
        "          data = json.load(json_file)\n",
        "  else:\n",
        "      data = {\"raw\": []}\n",
        "\n",
        "\n",
        "  data[\"raw\"].append({\"uri\": uri, \"raw_content\": raw_content})\n",
        "\n",
        "  with open(file_name, 'w', encoding='utf-8') as json_file:\n",
        "      json.dump(data, json_file, indent=4, ensure_ascii=False)"
      ],
      "metadata": {
        "id": "qEug_XP_evtj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def update_uri_state(uri, new_state):\n",
        "    file_name = 'uris.json'\n",
        "    if os.path.exists(file_name):\n",
        "        with open(file_name, 'r', encoding='utf-8') as json_file:\n",
        "            data = json.load(json_file)\n",
        "        if uri in data[\"uris\"]:\n",
        "            data[\"uris\"][uri] = new_state\n",
        "            with open(file_name, 'w', encoding='utf-8') as json_file:\n",
        "                json.dump(data, json_file, indent=4, ensure_ascii=False)"
      ],
      "metadata": {
        "id": "8qzoInre7J6c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_pending_uris():\n",
        "    file_name = 'uris.json'\n",
        "    pending_uris = []\n",
        "\n",
        "    if not os.path.exists(file_name):\n",
        "      uri_list = save_uris_pdfs_and_metadata_from_repositorio_unal()\n",
        "      return uri_list\n",
        "\n",
        "    with open(file_name, 'r', encoding='utf-8') as json_file:\n",
        "        data = json.load(json_file)\n",
        "        uris_dict = data.get(\"uris\", {})\n",
        "\n",
        "    for uri, state in uris_dict.items():\n",
        "        if state == \"pending\":\n",
        "            pending_uris.append(uri)\n",
        "\n",
        "    return pending_uris\n",
        "\n",
        "def get_error_uris():\n",
        "    file_name = 'uris.json'\n",
        "    error_uris = []\n",
        "\n",
        "    with open(file_name, 'r', encoding='utf-8') as json_file:\n",
        "        data = json.load(json_file)\n",
        "        uris_dict = data.get(\"uris\", {})\n",
        "\n",
        "    for uri, state in uris_dict.items():\n",
        "        if state == \"error\":\n",
        "            error_uris.append(uri)\n",
        "\n",
        "    return error_uris"
      ],
      "metadata": {
        "id": "PupQpAkQ7NhQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "CARGAR ARCHIVOS uris.json y raw_dataset.json SI EXISTEN\n"
      ],
      "metadata": {
        "id": "Zx9jAiN8K0Q9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "uri_list = get_pending_uris()"
      ],
      "metadata": {
        "id": "mYYbxgwpK6M4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(get_error_uris())"
      ],
      "metadata": {
        "id": "8z8aZVnGVfyR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0f5c695b-9883-4331-8b3d-9250885df4cb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "21"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(uri_list)"
      ],
      "metadata": {
        "id": "9COR7l5b3tSb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dba9a6ec-283c-44e4-fb03-01b85fb13c74"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Se extrae el contenido de cada PDF y se cambia el estado de 'pending' a 'complete'"
      ],
      "metadata": {
        "id": "uK7j-UjoZ-u4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def read_pdfs_by_uri_and_update_state():\n",
        "  for uri in uri_list:\n",
        "    print('INICIA uri: ' + uri)\n",
        "    url = 'https://repositorio.unal.edu.co'+uri\n",
        "    try:\n",
        "      raw_content = from_url_to_txt(url)\n",
        "    except:\n",
        "      update_uri_state(uri, 'error')\n",
        "\n",
        "    if raw_content != '':\n",
        "      print('FINALIZA uri: ' + uri+ ' len: '+ str(len(raw_content)))\n",
        "      save_content_in_json(uri, raw_content)\n",
        "      update_uri_state(uri, 'complete')\n",
        "    else:\n",
        "      print('ERROR URI: '+ uri)\n",
        "      update_uri_state(uri, 'error')\n"
      ],
      "metadata": {
        "id": "lMOFvKRL3ugh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "read_pdfs_by_uri_and_update_state()"
      ],
      "metadata": {
        "id": "-Bf4mQtJ6oUQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_list_raw_dataset():\n",
        "  file_name = 'raw_dataset.json'\n",
        "  raw_dataset = []\n",
        "  with open(file_name, 'r', encoding='utf-8') as json_file:\n",
        "    data = json.load(json_file)\n",
        "\n",
        "  return data"
      ],
      "metadata": {
        "id": "HRlAyIRR8iXf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def join_raw_data_to_dataset():\n",
        "  with open('dataset.json', 'r', encoding='utf-8') as dataset_file:\n",
        "    dataset = json.load(dataset_file)\n",
        "\n",
        "  with open('raw_dataset.json', 'r', encoding='utf-8') as raw_dataset_file:\n",
        "      raw_dataset = json.load(raw_dataset_file)\n",
        "\n",
        "  raw_content_mapping = {entry['uri']: entry['raw_content'] for entry in raw_dataset['raw']}\n",
        "\n",
        "  for uri, entry in dataset.items():\n",
        "      if uri in raw_content_mapping:\n",
        "          entry['raw_content'] = raw_content_mapping[uri]\n",
        "\n",
        "  with open('dataset.json', 'w', encoding='utf-8') as updated_dataset_file:\n",
        "      json.dump(dataset, updated_dataset_file, indent=4, ensure_ascii=False)"
      ],
      "metadata": {
        "id": "7bYXv2U9GSzE"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "join_raw_data_to_dataset()"
      ],
      "metadata": {
        "id": "BsAQXGqPi3m1"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('dataset.json', 'r') as dataset_file:\n",
        "    dataset = json.load(dataset_file)\n",
        "\n",
        "con_raw_data = 0\n",
        "sin_raw_data = 0\n",
        "\n",
        "for entry in dataset.values():\n",
        "    if 'raw_content' in entry:\n",
        "        con_raw_data += 1\n",
        "    else:\n",
        "        sin_raw_data += 1\n",
        "\n",
        "print(f\"Registros con 'raw_data': {con_raw_data}\")\n",
        "print(f\"Registros sin 'raw_data': {sin_raw_data}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "chYlz76z7rN-",
        "outputId": "65e91b8d-a85d-4d92-8550-ebed4095fc78"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Registros con 'raw_data': 1859\n",
            "Registros sin 'raw_data': 51\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#WIP\n",
        "def extract_content(text, index):\n",
        "    content_started = False\n",
        "    content = []\n",
        "\n",
        "    lines = text.split(\"\\n\")\n",
        "    i = 0\n",
        "    j =0\n",
        "    for line in lines:\n",
        "      if content_started:\n",
        "            j+=1\n",
        "            if j==100:\n",
        "              print('ERROR NO END')\n",
        "              return ['ERROR']\n",
        "\n",
        "            if \"referencias\" in line.strip().lower() or \"bibliograf\" in line.strip().lower():\n",
        "              break\n",
        "            if re.match(r'^\\d+\\.\\d+', line.lstrip()):\n",
        "              print('salto: '+line.lstrip())\n",
        "              continue\n",
        "            line= line.strip().lower().replace('.', '').lstrip('0123456789').rstrip('0123456789')\n",
        "            line = re.sub(r'\\b(?:i|ii|iii|iv|v|vi|vii|viii|ix|x)+\\b', '', line, flags=re.IGNORECASE).lstrip().rstrip()\n",
        "\n",
        "            if line != '' and 'anexo' not in line:\n",
        "              content.append(line)\n",
        "      i+=1\n",
        "      if i == 1000 and not content_started:\n",
        "        print('ERROR NOT FOUND')\n",
        "        return ['ERROR']\n",
        "\n",
        "      if  line.strip().lower().lstrip().rstrip().lstrip('0123456789').endswith(\"contenido\") or line.strip().lower().lstrip().rstrip().lstrip('0123456789').endswith('índice'):\n",
        "          content_started = True\n",
        "\n",
        "\n",
        "    content.append(str(index)+ '-------------------------------------------------------')\n",
        "    return content"
      ],
      "metadata": {
        "id": "JS7O47Mk6JUw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "raw_data = get_list_raw_dataset()['raw']\n"
      ],
      "metadata": {
        "id": "12vPB0pN8UiN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(extract_content(raw_data[0]['raw_content']))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ur3m30At-5ay",
        "outputId": "ddb964d9-9370-4608-fcf8-e02c3af05b3d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['contenido', 'pág', 'resumen', 'lista de figuras', 'lista de tablas', 'introducción', 'estadio alfonso lópez', 'historia', 'ubicación', 'estructura', 'criterios de diseño', 'normas aplicables', 'cancha de futbol', 'iluminación de emergencia', 'ubicación de luminarias', 'diseño de iluminación', 'características de las luminarias', 'factor de mantenimiento', 'resultados de diseño', 'disposición de luminarias', 'iluminación arquitectónica', 'control de iluminación', 'controlador mediante software', 'equipo transmisor', 'equipo receptor', 'conexión luminarias', 'análisis financiero', 'conclusiones y recomendaciones']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "content = []\n",
        "no_content_uris = []\n",
        "i=0\n",
        "c = ['ERROR']\n",
        "for raw in raw_data:\n",
        "  i+=1\n",
        "  if i == 100:\n",
        "    break\n",
        "  c = extract_content(raw['raw_content'], i)\n",
        "  if c[0] == 'ERROR':\n",
        "    no_content_uris.append(raw['uri'])\n",
        "  else:\n",
        "    for item in c:\n",
        "      if item not in content:\n",
        "        content.append(item)"
      ],
      "metadata": {
        "id": "LPKKi3hgBtDk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}